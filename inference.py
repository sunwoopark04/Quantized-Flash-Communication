import torch
import time

def run_qa_test(model, tokenizer, device, rank):
    """
    [QA Test] ì‹¤ì œ ëŒ€í™” í…ŒìŠ¤íŠ¸ ë° ì†ë„ ì¸¡ì •
    """
    # ë…¼ë¬¸ ì¬í˜„ ì„¤ì • (Flash INT6)
    for layer in model.model.layers:
        if hasattr(layer.mlp, "mode"):
            layer.mlp.mode = "flash"
            layer.mlp.bits = 6 
        
    prompt = "Explain london."
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    attention_mask = torch.ones_like(input_ids)
    
    if rank == 0:
        print(f"\nğŸ“ [QA Test] Prompt: {prompt}")
        
    torch.cuda.synchronize()
    
    # [ì¶”ê°€] ë§¤ë²ˆ ë‹¤ë¥¸ ë‹µë³€ì´ ë‚˜ì˜¤ë„ë¡, QA í…ŒìŠ¤íŠ¸ ì§ì „ì— ì‹œë“œë¥¼ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ë³€ê²½
    torch.manual_seed(int(time.time())) 
    
    start_t = time.perf_counter()
    
    with torch.no_grad():
        generated_ids = model.generate(
            input_ids, 
            attention_mask=attention_mask, 
            max_new_tokens=200,  # [ìœ ì§€] ë‹µë³€ì´ ëŠê¸°ì§€ ì•Šê²Œ ë„‰ë„‰íˆ ì„¤ì •
            do_sample=True, 
            temperature=0.7, 
            pad_token_id=tokenizer.pad_token_id
        )
    
    torch.cuda.synchronize()
    total_time = (time.perf_counter() - start_t) * 1000
        
    if rank == 0:
        output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        final_answer = output_text[len(prompt):].strip()
        
        print(f"ğŸ’¡ Answer:\n{final_answer}")
        print(f"ğŸš€ Total Generation Time: {total_time:.2f}ms")